name: Single-cell notebooks runner
run-name: Single-cell notebooks runner --- triggered by ${{ github.actor }}

"on":
  pull_request: {}
  push:
    branches: [main, master, staging, github-action-workflow, release/*]
  workflow_dispatch:
    inputs:
      environment:
        description: "Deploy Environment"
        required: true
        default: "staging"
      debug_from_step:
        description: 'Start from step (e.g. "load-docker")'
        required: false
        default: ""
      target_branch:
        description: "Branch to run on (e.g. main)"
        required: false
        default: ""

permissions:
  checks: write
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true


jobs:
  run-notebook:
    runs-on: arc-runner-set-oke-org-poc
    # environment: dev
    strategy:
      matrix:
        NOTEBOOK_FILENAME: [
            "01_scRNA_analysis_preprocessing.ipynb",
            # "02_scRNA_analysis_extended.ipynb"
            # "03_scRNA_analysis_with_pearson_residuals.ipynb",
            # "04_scRNA_analysis_dask_out_of_core.ipynb",
            # "05_scRNA_analysis_multi_GPU.ipynb",
            # "06_scRNA_analysis_90k_brain_example.ipynb",
            # "07_scRNA_analysis_1.3M_brain_example.ipynb",
          ]
      max-parallel: 4
      fail-fast: false

    env:
      DEPLOY_ENV: ${{ github.event_name == 'workflow_dispatch' && inputs.environment || 'dev' }}

      # Test image information
      TEST_IMAGE: "nvcr.io/rw983xdqtcdp/auto_test_team/blueprint-github-test-image:latest"

      #### VARS
      DOCKER_IMG_NAME: "nvcr.io/nvidia/rapidsai/notebooks"
      DOCKER_IMG_TAG: "25.06-cuda12.8-py3.12"
      DOCKER_COMPOSE_FILE: "${{ github.workspace }}/docker/brev/docker-compose-nb-2504.yaml"
      NOTEBOOK_RELATIVED_DIR: "notebooks"
      PYTHON_VERSION: "3.12"

      ### CONSTANT
      DOCKER_WRITEABLE_DIR: "/tmp"
      DOCKER_CACHE_DIR: "docker-pull-cache"
      OUTPUT_NOTEBOOK: "result_${{ matrix.NOTEBOOK_FILENAME }}"
      OUTPUT_NOTEBOOK_HTML: "result_${{ matrix.NOTEBOOK_FILENAME }}.html"
      OUTPUT_PYTEST_COVERAGE_XML: "pytest_coverage_${{ matrix.NOTEBOOK_FILENAME }}.xml"
      OUTPUT_PYTEST_RESULT_XML: "pytest_result_${{ matrix.NOTEBOOK_FILENAME }}.xml"
      OUTPUT_PYTEST_REPORT_HTML: "pytest_report_${{ matrix.NOTEBOOK_FILENAME }}.html"
      ARTIFACT_DIR: "${{ github.workspace }}/test-results/${{ matrix.NOTEBOOK_FILENAME }}"

    steps:
      - name: Set global vars
        id: set_global_vars
        run: |
          DOCKER_IMG_CACHE_NAME=$(echo "$DOCKER_IMG_NAME:$DOCKER_IMG_TAG" | sed 's/[\/:@.]/_/g')
          echo "Cache filename: ${DOCKER_IMG_CACHE_NAME}.tar"
          echo "docker_img_cache_name=${DOCKER_IMG_CACHE_NAME}" >> $GITHUB_OUTPUT

          NOTEBOOK_FILENAME="${{ matrix.NOTEBOOK_FILENAME }}"
          NOTEBOOK_BASENAME=$(basename "$NOTEBOOK_FILENAME" | cut -d. -f1)
          echo "notebook_filename=${NOTEBOOK_FILENAME}" >> $GITHUB_OUTPUT
          echo "notebook_basename=${NOTEBOOK_BASENAME}" >> $GITHUB_OUTPUT

          echo "cache_key=${{ runner.os }}-docker-$DOCKER_IMG_CACHE_NAME" >> $GITHUB_OUTPUT

          ARTIFACT_CLEAN_NAME=$(echo "$NOTEBOOK_BASENAME" | tr -d '"<>:|*?\\/')
          echo "artifact_name=results-$ARTIFACT_CLEAN_NAME" >> $GITHUB_OUTPUT

      - name: Checkout BP repository
        uses: actions/checkout@v4


      - uses: ./.github/actions/setup-env
        with:
          python-version: "3.12"
          check-disk: "false"

      - name: Setup Docker environment
        id: setup-docker-env
        uses: ./.github/actions/docker-env-setup
        with:
          ngc-api-key: ${{ secrets.NGC_API_KEY }}
          install-docker: true
          install-compose: true
          install-dependencies: true
          show-system-info: true
          update-packages: true
          enable-debug: false

      - name: Login to NGC and pull test image
        run: |
          echo "Logging into NGC registry..."
          echo "${{ secrets.NGC_API_KEY }}" | docker login nvcr.io -u '$oauthtoken' --password-stdin
          
          echo "Pulling test image..."
          docker pull ${{ env.TEST_IMAGE }}
          
          echo "Test image pulled successfully"
          docker images | grep "blueprint-github-test-image" || echo "Image not found in list"

      - uses: ./.github/actions/check-sysinfo

      - name: Manage Docker Image Cache
        id: docker-cache-manager
        uses: ./.github/actions/docker-cache
        with:
          image-name: ${{ env.DOCKER_IMG_NAME }}
          image-tag: ${{ env.DOCKER_IMG_TAG }}
          cache-dir: ${{ env.DOCKER_CACHE_DIR }}
          writeable-dir: ${{ env.DOCKER_WRITEABLE_DIR }}
          max-retries: 2
          timeout-minutes: 20
          runner-os: ${{ runner.os }}
          enable-debug: false

      - name: Start container
        id: start-container
        uses: ./.github/actions/container-manager
        with:
          action: start
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          wait-timeout: 300
          image-tag: ${{ env.DOCKER_IMG_TAG }}
          host-path: ${{ github.workspace }}
          enable-debug: false

      - name: Check container status and logs
        uses: ./.github/actions/container-manager
        with:
          action: status
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          log-lines: 200

      - name: Prepare container environment
        uses: ./.github/actions/container-manager
        with:
          action: prepare
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend

      - name: Install Python dependencies
        uses: ./.github/actions/container-manager
        with:
          action: exec
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          user: rapids
          working-dir: /home/rapids
          command: |
            echo 'Installing requirements'
            
            # Reset PATH to container defaults and add user paths
            export PATH="/opt/conda/bin:/usr/local/bin:/usr/bin:/bin:$HOME/.local/bin"
            export PYTHONPATH="$HOME/.local/lib/python3.12/site-packages:/opt/conda/lib/python3.12/site-packages"
            
            # Check current Python environment
            echo "Python version: $(python --version)"
            echo "Python executable: $(which python)"
            echo "Pip version: $(pip --version)"
            
            # Check if system packages are available (installed via EXTRA_PIP_PACKAGES)
            echo "Checking system packages installed via EXTRA_PIP_PACKAGES..."
            python -c "import scanpy; print(f'scanpy version: {scanpy.__version__}')" 2>/dev/null && echo "✅ scanpy found in system" || echo "❌ scanpy not found in system"
            python -c "import rapids_singlecell; print(f'rapids_singlecell version: {rapids_singlecell.__version__}')" 2>/dev/null && echo "✅ rapids_singlecell found in system" || echo "❌ rapids_singlecell not found in system"
            
            # Install essential tools if not already available
            echo "Installing additional tools..."
            python -c "import jupyter" 2>/dev/null && echo "✅ jupyter available" || pip install jupyter
            python -c "import papermill" 2>/dev/null && echo "✅ papermill available" || pip install papermill
            python -c "import pytest" 2>/dev/null && echo "✅ pytest available" || pip install pytest-html pytest-cov

            echo 'Verifying final installations'
            echo "=== System packages ==="
            pip list | grep -E "(scanpy|rapids-singlecell|jupyter|papermill)" || echo "Some packages not found"
            
            echo "=== User packages ==="
            pip list --user | grep -E "(scanpy|rapids-singlecell)" || echo "No user packages found"
            
            echo "=== Python import test ==="
            echo "Testing scanpy import..."
            python -c "import scanpy as sc; print(f'✅ scanpy {sc.__version__} imported successfully')" || echo "❌ scanpy import failed"
            
            echo "Testing rapids_singlecell import..."
            python -c "import rapids_singlecell as rsc; print(f'✅ rapids_singlecell {rsc.__version__} imported successfully')" || echo "❌ rapids_singlecell import failed"
            echo 'Verifying kernel'
            jupyter kernelspec list

      - name: Run Jupyter Notebook
        id: run-jupyter-notebook
        uses: ./.github/actions/container-exec
        with:
          action: notebook
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          user: rapids
          working-dir: /home/rapids
          notebook-input: ${{ env.NOTEBOOK_RELATIVED_DIR }}/${{ steps.set_global_vars.outputs.notebook_filename }}
          notebook-output: ${{ env.DOCKER_WRITEABLE_DIR }}/${{ env.OUTPUT_NOTEBOOK }}
          timeout-minutes: 30
          enable-debug: false

      - name: Convert result to html format
        id: generate-notebook-html
        if: steps.run-jupyter-notebook.outcome == 'success'
        uses: ./.github/actions/container-exec
        with:
          action: convert
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          user: rapids
          working-dir: ${{ env.DOCKER_WRITEABLE_DIR }}
          notebook-input: ${{ env.DOCKER_WRITEABLE_DIR }}/${{ env.OUTPUT_NOTEBOOK }}
          notebook-output: ${{ env.DOCKER_WRITEABLE_DIR }}/${{ env.OUTPUT_NOTEBOOK_HTML }}
          output-format: html
          timeout-minutes: 10
          enable-debug: false

      - name: Run pytest with test image
        id: run-pytest
        run: |
          set -euo pipefail
          
          # Prepare directories
          mkdir -p ${{ env.ARTIFACT_DIR }}
          NOTEBOOK_BASENAME="${{ steps.set_global_vars.outputs.notebook_basename }}"
          
          echo "Running pytest using test image: ${{ env.TEST_IMAGE }}"
          echo "Input notebook HTML: ${{ env.OUTPUT_NOTEBOOK_HTML }}"
          echo "Notebook basename: $NOTEBOOK_BASENAME"
          
          # Run pytest with the dedicated test image
          docker run --rm \
            -v "${{ github.workspace }}/test-results/${{ matrix.NOTEBOOK_FILENAME }}/${{ env.OUTPUT_NOTEBOOK_HTML }}":/app/input/${NOTEBOOK_BASENAME}.html \
            -v "${{ github.workspace }}:/workspace" \
            ${{ env.TEST_IMAGE }} \
            pytest -m "single_cell" \
              --disable-warnings \
              --html=/workspace/test-results/${{ matrix.NOTEBOOK_FILENAME }}/${{ env.OUTPUT_PYTEST_REPORT_HTML }} \
              --self-contained-html \
              --junitxml=/workspace/test-results/${{ matrix.NOTEBOOK_FILENAME }}/${{ env.OUTPUT_PYTEST_RESULT_XML }} \
              || echo "Pytest completed with exit code $?"
          
          # Verify test outputs were created
          echo "Checking test outputs:"
          ls -la ${{ env.ARTIFACT_DIR }}/
          
          # Create a simple results summary
          if [ -f "${{ env.ARTIFACT_DIR }}/${{ env.OUTPUT_PYTEST_RESULT_XML }}" ]; then
            echo "Test results XML found"
            echo "test_results_available=true" >> $GITHUB_OUTPUT
          else
            echo "Test results XML not found"
            echo "test_results_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Process test results
        id: process-test-results
        if: steps.run-pytest.outputs.test_results_available == 'true'
        run: |
          set -euo pipefail
          
          XML_FILE="${{ env.ARTIFACT_DIR }}/${{ env.OUTPUT_PYTEST_RESULT_XML }}"
          HTML_FILE="${{ env.ARTIFACT_DIR }}/${{ env.OUTPUT_PYTEST_REPORT_HTML }}"
          
          if [ ! -f "$XML_FILE" ]; then
            echo "ERROR: Test results XML file not found: $XML_FILE"
            echo "Available files:"
            ls -la ${{ env.ARTIFACT_DIR }}/
            exit 1
          fi
          
          echo "Processing test results from: $XML_FILE"
          
          # Extract test metrics using xmllint
          parse_xml() {
            local xpath="$1"
            xmllint --xpath "$xpath" "$XML_FILE" 2>/dev/null || echo "0"
          }
          
          passed=$(parse_xml 'count(//testcase[not(skipped) and not(failure) and not(error)])')
          failures=$(parse_xml 'count(//testcase[failure])')
          errors=$(parse_xml 'count(//testcase[error])')
          skipped=$(parse_xml 'count(//testcase[skipped])')
          
          echo "passed=$passed" >> $GITHUB_OUTPUT
          echo "failed=$failures" >> $GITHUB_OUTPUT
          echo "skipped=$skipped" >> $GITHUB_OUTPUT
          echo "errors=$errors" >> $GITHUB_OUTPUT
          echo "xfailed=0" >> $GITHUB_OUTPUT
          echo "xpassed=0" >> $GITHUB_OUTPUT
          
          # Create test results JSON
          TEST_RESULTS_JSON=$(cat << EOF
          {
            "passed": $passed,
            "failed": $failures,
            "skipped": $skipped,
            "errors": $errors,
            "total": $((passed + failures + skipped + errors))
          }
          EOF
          )
          
          echo "test_results_json<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_RESULTS_JSON" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "### Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "- Passed: $passed" >> $GITHUB_STEP_SUMMARY
          echo "- Failed: $failures" >> $GITHUB_STEP_SUMMARY
          echo "- Errors: $errors" >> $GITHUB_STEP_SUMMARY
          echo "- Skipped: $skipped" >> $GITHUB_STEP_SUMMARY
          echo "- Total: $((passed + failures + skipped + errors))" >> $GITHUB_STEP_SUMMARY

      - name: Copy notebook HTML result from container
        id: copy-notebook-html
        if: steps.generate-notebook-html.outcome == 'success'
        uses: ./.github/actions/container-exec
        with:
          action: copy-from
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend
          source-path: ${{ env.DOCKER_WRITEABLE_DIR }}/${{ env.OUTPUT_NOTEBOOK_HTML }}
          dest-path: ${{ env.ARTIFACT_DIR }}

      - name: Generate result metadata
        run: |
          ARTIFACT_DIR="${{ env.ARTIFACT_DIR }}"
          TEST_RESULTS='${{ steps.process-test-results.outputs.test_results_json }}'
          if [ -z "$TEST_RESULTS" ]; then
            TEST_RESULTS='{}'
          fi
          echo "$TEST_RESULTS" > "$ARTIFACT_DIR/${{ matrix.NOTEBOOK_FILENAME }}.json"

          mkdir -p "${{ github.workspace }}/json-results"
          echo "$TEST_RESULTS" > "json-results/${{ matrix.NOTEBOOK_FILENAME }}.json"

          echo "INFO: Contents of artifact directory:"
          ls -al "$ARTIFACT_DIR"
          echo "INFO: Contents of json-results directory:"
          ls -al "${{ github.workspace }}/json-results"

      - name: Upload the result notebook as artifact
        if: always() && (steps.copy-notebook-html.outcome == 'success' || steps.run-pytest.outcome == 'success')
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.set_global_vars.outputs.artifact_name }}
          path: ${{ env.ARTIFACT_DIR }}/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload JSON test results
        if: steps.process-test-results.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: json-results-${{ matrix.NOTEBOOK_FILENAME }}
          path: json-results/${{ matrix.NOTEBOOK_FILENAME }}.json
          retention-days: 30

      - name: Stop containers
        if: ${{ always() }}
        uses: ./.github/actions/container-manager
        with:
          action: stop
          compose-file: ${{ env.DOCKER_COMPOSE_FILE }}
          service-name: backend

      - name: Clean up
        if: always()
        run: |
          docker system prune -f

  summary:
    needs: run-notebook
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Query cache via API
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          for i in {1..3}; do
            echo "Attempt $i: Querying cache..."
            response=$(curl -s \
              -H "Authorization: Bearer $GITHUB_TOKEN" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/caches?key=${{ runner.os }}-docker-")

            if echo "$response" | grep -q "${{ runner.os }}-docker-"; then
              echo "$response" | jq .
              exit 0
            else
              echo "Cache not found. Retrying in 10 seconds..."
              sleep 10
            fi
          done
          echo "Error: Cache not found after 3 attempts."
          exit 1

      - name: Download JSON test results
        uses: actions/download-artifact@v4
        with:
          path: downloaded-results

      - name: Generate final report
        run: |
          echo "### 📊 Notebook Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "| Notebook | ✅ Passed | ❌ Failed | ⏩ Skipped | 💥 Errors |" >> $GITHUB_STEP_SUMMARY
          echo "|----------|----------|----------|-----------|----------|" >> $GITHUB_STEP_SUMMARY

          notebooks=(
            "01_scRNA_analysis_preprocessing.ipynb"
            "02_scRNA_analysis_extended.ipynb"
            "03_scRNA_analysis_with_pearson_residuals.ipynb"
            "04_scRNA_analysis_dask_out_of_core.ipynb"
            "05_scRNA_analysis_multi_GPU.ipynb"
            "06_scRNA_analysis_90k_brain_example.ipynb"
            "07_scRNA_analysis_1.3M_brain_example.ipynb"
          )

          total_passed=0
          total_failed=0
          total_skipped=0
          total_errors=0
          total_xfailed=0
          total_xpassed=0

          for notebook in "${notebooks[@]}"; do
            nb_name=$(basename "$notebook")
            json_file="downloaded-results/json-results-${notebook}/$notebook.json"

            if [ -f "$json_file" ]; then
              results=$(cat "$json_file")

              # Extract test results to avoid complex nesting
              passed=$(echo "$results" | jq -r '.passed')
              failed=$(echo "$results" | jq -r '.failed')
              skipped=$(echo "$results" | jq -r '.skipped')
              errors=$(echo "$results" | jq -r '.errors')
              xfailed=$(echo "$results" | jq -r '.xfailed')
              xpassed=$(echo "$results" | jq -r '.xpassed')
              
              # Update totals
              total_passed=$((total_passed + passed))
              total_failed=$((total_failed + failed))
              total_skipped=$((total_skipped + skipped))
              total_errors=$((total_errors + errors))
              total_xfailed=$((total_xfailed + xfailed))
              total_xpassed=$((total_xpassed + xpassed))

              echo "| $nb_name | $passed | $failed | $skipped | $errors |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| $nb_name | ❌ Missing data | ❌ Missing data | ❌ Missing data | ❌ Missing data |" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "| **Total** | **$total_passed** | **$total_failed** | **$total_skipped** | **$total_errors** |" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 📌 Additional Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- 🔶 Expected Failures (XFailed): $total_xfailed" >> $GITHUB_STEP_SUMMARY
          echo "- ⚠️ Unexpected Passes (XPassed): $total_xpassed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📊 [View Detailed Artifacts]($GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID)" >> $GITHUB_STEP_SUMMARY
