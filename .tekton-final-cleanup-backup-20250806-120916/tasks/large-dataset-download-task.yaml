apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: large-dataset-download-task
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/name: large-dataset-download-task
    app.kubernetes.io/component: tekton-task
    app.kubernetes.io/version: "1.0.0"
spec:
  description: |
    Downloads large scientific datasets for single-cell analysis.
    Based on step3-download-scientific-dataset from the reference workflow.
    
  params:
  - name: dataset-urls
    type: string
    description: URLs of datasets to download (one per line)
  - name: dataset-names
    type: string
    description: Names for the datasets (comma-separated)
  - name: download-timeout
    type: string
    default: "600"
    description: Download timeout in seconds
    
  workspaces:
  - name: shared-storage
    description: Workspace to store downloaded datasets
  - name: dataset-cache
    description: Cache workspace for datasets
    
  steps:
  - name: download-datasets
    image: alpine:latest
    script: |
      #!/bin/sh
      set -eu
      
      echo "üì• Step 3: Downloading Scientific Datasets"
      echo "=========================================="
      
      cd "$(workspaces.shared-storage.path)"
      
      # Install wget if not available
      apk add --no-cache wget curl
      
      # Create data directories
      mkdir -p h5 data datasets
      
      # Parse dataset URLs and names
      URLS="$(params.dataset-urls)"
      NAMES="$(params.dataset-names)"
      TIMEOUT="$(params.download-timeout)"
      
      echo "Dataset URLs: $URLS"
      echo "Dataset Names: $NAMES" 
      echo "Timeout: $TIMEOUT seconds"
      
      # Download datasets
      echo "$URLS" | while IFS= read -r url; do
        if [ -n "$url" ]; then
          filename=$(basename "$url")
          echo "üì• Downloading: $url"
          echo "   Filename: $filename"
          
          # Download with timeout
          if timeout "$TIMEOUT" wget -q --show-progress -O "h5/$filename" "$url"; then
            echo "‚úÖ Downloaded: $filename"
            
            # Verify file size
            size=$(du -h "h5/$filename" | cut -f1)
            echo "   Size: $size"
          else
            echo "‚ùå Failed to download: $url"
            echo "‚ö†Ô∏è  Continuing with other downloads..."
          fi
        fi
      done
      
      echo ""
      echo "üìã Download Summary:"
      echo "==================="
      ls -lh h5/ || echo "No files downloaded"
      
      echo "‚úÖ Dataset download completed"