apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: pytest-execution
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/name: pytest-execution
    app.kubernetes.io/component: tekton-task
    app.kubernetes.io/version: "1.0.0"
spec:
  description: |
    Executes pytest testing framework on HTML files.
    Based on step7-pytest-execution from the reference workflow.
    
  params:
  - name: html-input-file
    type: string
    description: HTML file to test
    
  workspaces:
  - name: shared-storage
    description: Workspace containing test files and framework
    
  steps:
  - name: execute-pytest
    image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
    securityContext:
      runAsUser: 0
    env:
    - name: HTML_INPUT_FILE
      value: $(params.html-input-file)
    script: |
      #!/bin/bash
      set -eu
      
      echo "üß™ Step 7: PyTest Execution"
      echo "==========================="
      
      cd "$(workspaces.shared-storage.path)"
      
      # Install pytest and coverage tools
      pip install --quiet --no-cache-dir pytest pytest-html pytest-cov coverage
      
      echo "HTML input file: ${HTML_INPUT_FILE}"
      
      # Check if HTML file exists
      if [ ! -f "${HTML_INPUT_FILE}" ]; then
        echo "‚ùå ERROR: HTML input file not found: ${HTML_INPUT_FILE}"
        echo "üìÇ Available files in input directory:"
        ls -la input/ || echo "Input directory does not exist"
        exit 1
      fi
      
      # Check if test framework exists
      if [ ! -d "blueprint-github-test" ]; then
        echo "‚ùå ERROR: Test framework not found: blueprint-github-test"
        echo "üìÇ Available directories:"
        ls -la | grep "^d"
        exit 1
      fi
      
      # Set up test environment
      export HTML_INPUT_FILE="${HTML_INPUT_FILE}"
      
      # Create artifacts directory
      mkdir -p artifacts
      
      # Define output files
      OUTPUT_PYTEST_REPORT_HTML="artifacts/pytest_report.html"
      OUTPUT_PYTEST_RESULT_XML="artifacts/pytest_results.xml"
      OUTPUT_PYTEST_COVERAGE_XML="artifacts/coverage.xml"
      
      echo "üß™ Running pytest..."
      echo "Test framework: blueprint-github-test"
      echo "Output report: ${OUTPUT_PYTEST_REPORT_HTML}"
      echo "Output results: ${OUTPUT_PYTEST_RESULT_XML}"
      echo "Coverage report: ${OUTPUT_PYTEST_COVERAGE_XML}"
      
      # Execute pytest with comprehensive reporting
      # Run pytest with proper error handling
      set +e  # Allow pytest to fail
      pytest blueprint-github-test/ \
          --html="${OUTPUT_PYTEST_REPORT_HTML}" \
          --self-contained-html \
          --junitxml="${OUTPUT_PYTEST_RESULT_XML}" \
          --cov=. \
          --cov-report=xml:"${OUTPUT_PYTEST_COVERAGE_XML}" \
          --cov-report=html:artifacts/htmlcov \
          -v \
          --tb=short 2>&1 | tee artifacts/pytest_execution.log
      PYTEST_EXIT_CODE=$?
      set -e
      
      # Check for critical errors
      if grep -q "ModuleNotFoundError\|ImportError.*playwright" artifacts/pytest_execution.log 2>/dev/null; then
        echo "‚ùå Critical dependency missing (playwright)"
        echo "üìä Installing missing dependencies..."
        
        # Try to install playwright
        pip install --user playwright pytest-playwright || {
          echo "‚ùå Failed to install playwright dependencies"
          echo "üí° This indicates a configuration issue that needs to be resolved"
          exit 1
        }
        
        # Try to install playwright browsers
        python -m playwright install chromium || {
          echo "‚ö†Ô∏è Failed to install playwright browsers, but continuing..."
        }
        
        # Retry pytest with dependencies installed
        echo "üîÑ Retrying pytest with dependencies..."
        pytest blueprint-github-test/ \
            --html="${OUTPUT_PYTEST_REPORT_HTML}" \
            --self-contained-html \
            --junitxml="${OUTPUT_PYTEST_RESULT_XML}" \
            --cov=. \
            --cov-report=xml:"${OUTPUT_PYTEST_COVERAGE_XML}" \
            --cov-report=html:artifacts/htmlcov \
            -v \
            --tb=short 2>&1 | tee -a artifacts/pytest_execution.log
        PYTEST_EXIT_CODE=$?
      fi
      
      # Evaluate final results
      if [ $PYTEST_EXIT_CODE -eq 0 ]; then
        echo "‚úÖ Pytest completed successfully"
      elif [ $PYTEST_EXIT_CODE -eq 5 ]; then
        echo "‚ö†Ô∏è No tests were collected (this may be expected)"
      else
        echo "‚ùå Pytest failed with exit code: $PYTEST_EXIT_CODE"
        echo "üìä Check pytest_execution.log for details"
        # For now, treat test failures as warnings, not errors
        echo "‚ö†Ô∏è Continuing with warnings (test failures may be expected)"
      fi
      
      # Verify outputs
      echo ""
      echo "üìã Generated Test Artifacts:"
      echo "============================"
      for file in "${OUTPUT_PYTEST_REPORT_HTML}" "${OUTPUT_PYTEST_RESULT_XML}" "${OUTPUT_PYTEST_COVERAGE_XML}"; do
        if [ -f "$file" ]; then
          size=$(du -h "$file" | cut -f1)
          echo "‚úÖ $file (${size})"
        else
          echo "‚ùå Missing: $file"
        fi
      done
      
      echo "‚úÖ Pytest execution completed"