apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: pytest-execution
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/name: pytest-execution
    app.kubernetes.io/component: tekton-task
    task.tekton.dev/test-execution: "true"
spec:
  description: |
    Execute pytest tests on HTML notebook outputs with comprehensive reporting.
    
    Features:
    - HTML-based notebook testing
    - Coverage analysis with XML reports
    - Test result collection in multiple formats
    - Error handling and detailed logging
    
  workspaces:
  - name: shared-storage
    description: Main workspace containing test files and outputs
    mountPath: /workspace/shared
  
  params:
  - name: html-input-file
    description: HTML notebook file to test
    type: string
  - name: test-markers
    description: Pytest markers to run (default: single_cell)
    type: string
    default: "single_cell"
  - name: coverage-source
    description: Source directory for coverage analysis
    type: string
    default: "."
  - name: continue-on-failure
    description: Continue pipeline even if tests fail
    type: string
    default: "true"
  
  results:
  - name: test-status
    description: Overall test execution status
  - name: test-count
    description: Number of tests executed
  - name: coverage-percentage
    description: Code coverage percentage
  
  steps:
  - name: setup-test-environment
    image: python:3.12-slim
    securityContext:
      runAsUser: 0
    script: |
      #!/bin/bash
      set -eu
      
      echo "🧪 Setting up pytest environment"
      echo "================================"
      
      cd $(workspaces.shared-storage.path)
      
      # Install pytest and related packages
      pip install --quiet pytest pytest-html pytest-cov beautifulsoup4
      
      # Check if test framework repository exists
      if [ ! -d "blueprint-github-test" ]; then
        echo "⚠️ Test framework not found, creating basic test structure"
        mkdir -p blueprint-github-test/tests
        
        # Create a basic test file for HTML validation
        cat > blueprint-github-test/tests/test_html_validation.py << 'EOF'
import pytest
from pathlib import Path
import re
from bs4 import BeautifulSoup


def test_html_file_exists():
    """Test that the HTML file exists and is readable."""
    html_file = Path("$(params.html-input-file)")
    assert html_file.exists(), f"HTML file {html_file} not found"
    assert html_file.stat().st_size > 0, f"HTML file {html_file} is empty"


def test_html_content_valid():
    """Test that the HTML content is valid and contains expected elements."""
    html_file = Path("$(params.html-input-file)")
    if not html_file.exists():
        pytest.skip(f"HTML file {html_file} not found")
    
    with open(html_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Basic HTML structure checks
    assert '<html' in content.lower(), "HTML file missing <html> tag"
    assert '<body' in content.lower(), "HTML file missing <body> tag"
    
    # Check for notebook content indicators
    soup = BeautifulSoup(content, 'html.parser')
    assert soup.title is not None, "HTML file missing title"


@pytest.mark.single_cell
def test_notebook_execution_indicators():
    """Test for indicators that the notebook was executed successfully."""
    html_file = Path("$(params.html-input-file)")
    if not html_file.exists():
        pytest.skip(f"HTML file {html_file} not found")
    
    with open(html_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Check for common single-cell analysis indicators
    indicators = [
        'cell', 'gene', 'analysis', 'data', 'preprocessing'
    ]
    
    found_indicators = []
    for indicator in indicators:
        if indicator.lower() in content.lower():
            found_indicators.append(indicator)
    
    assert len(found_indicators) >= 2, f"Expected at least 2 analysis indicators, found: {found_indicators}"


@pytest.mark.single_cell
def test_output_artifacts():
    """Test that expected output artifacts are present."""
    artifacts_dir = Path("artifacts")
    if artifacts_dir.exists():
        artifact_files = list(artifacts_dir.glob("*"))
        assert len(artifact_files) > 0, "No artifacts found in artifacts directory"
        
        # Check for common output file types
        file_extensions = [f.suffix for f in artifact_files]
        expected_extensions = ['.ipynb', '.html', '.log']
        found_extensions = [ext for ext in expected_extensions if ext in file_extensions]
        
        assert len(found_extensions) > 0, f"No expected artifact types found. Available: {file_extensions}"
EOF
      fi
      
      echo "✅ Test environment setup complete"
  
  - name: execute-pytest
    image: python:3.12-slim
    script: |
      #!/bin/bash
      set -eu
      
      echo "🧪 Executing pytest tests"
      echo "========================="
      
      cd $(workspaces.shared-storage.path)
      
      # Ensure pytest is available
      pip install --quiet pytest pytest-html pytest-cov beautifulsoup4
      
      # Set up test parameters
      HTML_FILE="$(params.html-input-file)"
      TEST_MARKERS="$(params.test-markers)"
      CONTINUE_ON_FAILURE="$(params.continue-on-failure)"
      
      echo "📋 Test Configuration:"
      echo "  HTML File: $HTML_FILE"
      echo "  Test Markers: $TEST_MARKERS"
      echo "  Continue on Failure: $CONTINUE_ON_FAILURE"
      
      # Create results directory
      mkdir -p pytest_results
      
      # Run pytest with comprehensive reporting
      set +e  # Don't exit on pytest failures if continue-on-failure is true
      
      if [ -d "blueprint-github-test" ]; then
        cd blueprint-github-test
        
        pytest tests/ \
          -v \
          --tb=short \
          --html=../pytest_results/pytest_report.html \
          --self-contained-html \
          --cov=. \
          --cov-report=xml:../pytest_results/coverage.xml \
          --cov-report=html:../pytest_results/htmlcov \
          --junitxml=../pytest_results/pytest_results.xml \
          -m "$TEST_MARKERS" \
          2>&1 | tee ../pytest_results/pytest.log
        
        PYTEST_EXIT_CODE=$?
        cd ..
      else
        echo "⚠️ No test directory found, creating minimal test"
        PYTEST_EXIT_CODE=0
      fi
      
      set -e
      
      # Copy results to main directory for pipeline access
      cp pytest_results/* . 2>/dev/null || echo "No results to copy"
      
      # Generate summary
      echo "📊 Test Execution Summary:"
      if [ $PYTEST_EXIT_CODE -eq 0 ]; then
        echo "✅ All tests passed"
        echo "passed" > $(results.test-status.path)
      else
        echo "⚠️ Some tests failed (exit code: $PYTEST_EXIT_CODE)"
        if [ "$CONTINUE_ON_FAILURE" = "true" ]; then
          echo "✅ Continuing pipeline as configured"
          echo "passed-with-warnings" > $(results.test-status.path)
        else
          echo "❌ Stopping pipeline due to test failures"
          echo "failed" > $(results.test-status.path)
          exit $PYTEST_EXIT_CODE
        fi
      fi
      
      # Extract test count and coverage
      if [ -f "pytest_results.xml" ]; then
        TEST_COUNT=$(grep -o 'tests="[0-9]*"' pytest_results.xml | cut -d'"' -f2 || echo "0")
        echo "$TEST_COUNT" > $(results.test-count.path)
      else
        echo "0" > $(results.test-count.path)
      fi
      
      if [ -f "coverage.xml" ]; then
        COVERAGE=$(grep -o 'line-rate="[0-9.]*"' coverage.xml | head -1 | cut -d'"' -f2 | awk '{printf "%.1f", $1*100}' || echo "0")
        echo "$COVERAGE%" > $(results.coverage-percentage.path)
      else
        echo "N/A" > $(results.coverage-percentage.path)
      fi
      
      echo "✅ Pytest execution completed"